{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hUjzgl9NqM6",
        "outputId": "c7af57fd-7926-4607-d6e0-bdb19831080a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          word  similarity_1_2  similarity_2_3  similarity_3_4\n",
            "0         Nice        0.365742        0.588511        1.000000\n",
            "1         Girl        0.258573        0.336131        0.021396\n",
            "2          Gay        0.021771        0.017718        0.000000\n",
            "3        Awful       -0.035649       -0.042638        0.595060\n",
            "4  Manufacture        0.723256        0.233119        0.423240\n",
            "5      Villain        0.333270        0.262052        0.641448\n",
            "6         Meat        0.279764        0.692848        0.615773\n",
            "7        Silly        0.682956        0.583736       -0.067588\n",
            "8       Cursor        0.629800        0.599075        1.000000\n",
            "9          Guy        0.295790        0.055928       -0.027981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-fbac2a9452dc>:12: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tokenized_definitions = [word_tokenize(definition.lower()) for definition in df['first_definition'].append(df['second_definition'])]\n"
          ]
        }
      ],
      "source": [
        "#Word2Vec\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "df = pd.read_csv('')\n",
        "\n",
        "\n",
        "tokenized_definitions = [word_tokenize(definition.lower()) for definition in df['first_definition'].append(df['second_definition'])]\n",
        "\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_definitions, vector_size=300, window=2, min_count=1, workers=4)\n",
        "\n",
        "\n",
        "def calculate_similarity(definition1, definition2):\n",
        "    tokens_definition1 = word_tokenize(definition1.lower())\n",
        "    tokens_definition2 = word_tokenize(definition2.lower())\n",
        "\n",
        "\n",
        "    tokens_definition1 = [token for token in tokens_definition1 if token in model.wv.key_to_index]\n",
        "    tokens_definition2 = [token for token in tokens_definition2 if token in model.wv.key_to_index]\n",
        "\n",
        "    if tokens_definition1 and tokens_definition2:\n",
        "        embedding_definition1 = model.wv[tokens_definition1].mean(axis=0)\n",
        "        embedding_definition2 = model.wv[tokens_definition2].mean(axis=0)\n",
        "\n",
        "\n",
        "        similarity = cosine_similarity([embedding_definition1], [embedding_definition2])[0][0]\n",
        "        return similarity\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    definition1 = row['first_definition']\n",
        "    definition2 = row['second_definition']\n",
        "    definition3 = row['third_definition']\n",
        "    definition4 = row['fourth_definition']\n",
        "\n",
        "\n",
        "    similarity_1_2 = calculate_similarity(definition1, definition2)\n",
        "    similarity_2_3 = calculate_similarity(definition2, definition3)\n",
        "    similarity_3_4 = calculate_similarity(definition3, definition4)\n",
        "\n",
        "    similarities.append((similarity_1_2, similarity_2_3, similarity_3_4))\n",
        "\n",
        "\n",
        "df['similarity_1_2'], df['similarity_2_3'], df['similarity_3_4'] = zip(*similarities)\n",
        "\n",
        "\n",
        "print(df[['word', 'similarity_1_2', 'similarity_2_3', 'similarity_3_4']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhFpSW0NGrDf"
      },
      "outputs": [],
      "source": [
        "#BERT\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "df = pd.read_csv('')  # Update with your actual file path\n",
        "\n",
        "\n",
        "def calculate_similarity(definition1, definition2):\n",
        "    tokens_definition1 = tokenizer(definition1, return_tensors=\"pt\")\n",
        "    outputs_definition1 = model(**tokens_definition1)\n",
        "    embedding_definition1 = outputs_definition1.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "    tokens_definition2 = tokenizer(definition2, return_tensors=\"pt\")\n",
        "    outputs_definition2 = model(**tokens_definition2)\n",
        "    embedding_definition2 = outputs_definition2.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "    similarity = cosine_similarity(embedding_definition1, embedding_definition2)[0][0]\n",
        "    return similarity\n",
        "\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    definition1 = row['first_definition']\n",
        "    definition2 = row['second_definition']\n",
        "    definition3 = row['third_definition']\n",
        "    definition4 = row['fourth_definition']\n",
        "\n",
        "\n",
        "    similarity_1_2 = calculate_similarity(definition1, definition2)\n",
        "    similarity_2_3 = calculate_similarity(definition2, definition3)\n",
        "    similarity_3_4 = calculate_similarity(definition3, definition4)\n",
        "\n",
        "    similarities.append((similarity_1_2, similarity_2_3, similarity_3_4))\n",
        "\n",
        "df['similarity_1_2'], df['similarity_2_3'], df['similarity_3_4'] = zip(*similarities)\n",
        "\n",
        "\n",
        "print(df[['word', 'similarity_1_2', 'similarity_2_3', 'similarity_3_4']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca9m05tmG1-Z"
      },
      "outputs": [],
      "source": [
        "#GloVe\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "\n",
        "df = pd.read_csv('')  # Update with your actual file path\n",
        "\n",
        "\n",
        "def calculate_similarity(definition1, definition2):\n",
        "    embedding_definition1 = nlp(definition1).vector\n",
        "    embedding_definition2 = nlp(definition2).vector\n",
        "\n",
        "\n",
        "    similarity = cosine_similarity([embedding_definition1], [embedding_definition2])[0][0]\n",
        "    return similarity\n",
        "\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    definition1 = row['first_definition']\n",
        "    definition2 = row['second_definition']\n",
        "    definition3 = row['third_definition']\n",
        "    definition4 = row['fourth_definition']\n",
        "\n",
        "\n",
        "    similarity_1_2 = calculate_similarity(definition1, definition2)\n",
        "    similarity_2_3 = calculate_similarity(definition2, definition3)\n",
        "    similarity_3_4 = calculate_similarity(definition3, definition4)\n",
        "\n",
        "    similarities.append((similarity_1_2, similarity_2_3, similarity_3_4))\n",
        "\n",
        "df['similarity_1_2'], df['similarity_2_3'], df['similarity_3_4'] = zip(*similarities)\n",
        "\n",
        "\n",
        "print(df[['word', 'similarity_1_2', 'similarity_2_3', 'similarity_3_4']])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
